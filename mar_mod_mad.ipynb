{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LiChaidoescode/March-Model-Madness/blob/main/NSDC_March_Madness_Classification_Outline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "28b9c81a",
      "metadata": {
        "id": "28b9c81a"
      },
      "source": [
        "### March Model Madness - a straightforward approach to predictive algorithms  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c33e8e44",
      "metadata": {
        "id": "c33e8e44"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "import sklearn as sk\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import KFold \n",
        "from sklearn.model_selection import cross_val_score\n",
        "import csv\n",
        "import random\n",
        "import numpy\n",
        "import seaborn as sb"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "409ef523",
      "metadata": {
        "id": "409ef523"
      },
      "source": [
        "The first thing was to import the most recent regular season team-level statistics along team-level box scores for many regular seasons of historical data, starting with the 2003 season (men). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f336b235",
      "metadata": {
        "id": "f336b235"
      },
      "outputs": [],
      "source": [
        "mm_tourney = pd.read_csv(\"mmmData/MNCAATourneyDetailedResults.csv\")\n",
        "mm_tourney = mm_tourney.dropna(axis = 0)\n",
        "mm_tourney\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1176b460",
      "metadata": {},
      "outputs": [],
      "source": [
        "mm_regseason = pd.read_csv(\"mmmData/MRegularSeasonDetailedResults.csv\")\n",
        "mm_regseason = mm_regseason.dropna(axis = 0)\n",
        "mm_regseason.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "274a0824",
      "metadata": {
        "id": "274a0824"
      },
      "outputs": [],
      "source": [
        "corrM1 = mm_tourney.corr()\n",
        "corrM2 = mm_regseason.corr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2e668e7",
      "metadata": {
        "id": "e2e668e7",
        "outputId": "d92d928c-ef9d-49fc-f7af-9de4b41e774b"
      },
      "outputs": [],
      "source": [
        "sb.heatmap(corrM1, cmap = \"Blues\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c685f4e9",
      "metadata": {},
      "outputs": [],
      "source": [
        "sb.heatmap(corrM2, cmap = \"crest\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "a5d5d792",
      "metadata": {
        "id": "a5d5d792"
      },
      "source": [
        "This correlation matrix only represents numerical values in the data, so things such as team names and rounds in the bracket are not displayed. I followed this step from the NSDC March Madness Classification outline. By inspection, most of the correlated box level scores are quite obvious. For example, field goals made by a losing team is pretty highly correlated with defensive rebounds made by the winning team. Originally, my hopes were to determine which statistics would be the most highly correlated to create a sleeker model. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "2a9085fb",
      "metadata": {
        "id": "2a9085fb"
      },
      "source": [
        "After reviewing a number of old march madness projects I decided to take the path taken by Matt Harvey. He used recent team performance and ELO ratings to predict tournament performance. I've used his work as a starting point, updated the data used, and incorporated additional statistical learning models. According to my research, support vector machines are better for this type of highly variable data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54c1e2ff",
      "metadata": {
        "id": "54c1e2ff",
        "outputId": "6c52c5c7-ba50-4eda-d2c8-280577a378d3"
      },
      "outputs": [],
      "source": [
        "# Initalize a few of the variables and arrays that are used by multiple functions. \n",
        "base_elo = 1600\n",
        "team_elos = {}  # Reset each year.\n",
        "team_stats = {}\n",
        "X = []\n",
        "y = []\n",
        "submission_data = []\n",
        "seeds = pd.read_csv('mmmData/MNCAATourneySeeds.csv')\n",
        "names = pd.read_csv('mmmData/MTeams.csv')\n",
        "seeds = seeds.merge(names[['TeamName']], on='TeamID')\n",
        "prediction_year = 2023\n",
        "folder = 'mmmData'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08269fa6",
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_data():\n",
        "    for i in range(1985, prediction_year+1):\n",
        "        team_elos[i] = {}\n",
        "        team_stats[i] = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96cb14cf",
      "metadata": {},
      "outputs": [],
      "source": [
        "# The purpose of this function is to retrieve the ELO ranking of a specific team. \n",
        "# If an error occurs on the return, the function retrives the previous season's ending\n",
        "# value. If an error occurs there it assisngs a base elo ranking. \n",
        "def get_elo(season, team):\n",
        "    try:\n",
        "        return team_elos[season][team]\n",
        "    except:\n",
        "        try:\n",
        "            # Get the previous season's ending value.\n",
        "            team_elos[season][team] = team_elos[season - 1][team]\n",
        "            return team_elos[season][team]\n",
        "        except:\n",
        "            # Get the starter elo.\n",
        "\n",
        "            team_elos[season][team] = base_elo\n",
        "            \n",
        "            return team_elos[season][team]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d717d42f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Given two teams and the season, this function determines the ELO rating. \n",
        "# The link is broken but I got this function from Matt Harvey's work. \n",
        "def calc_elo(win_team, lose_team, season):\n",
        "    winner_rank = get_elo(season, win_team)\n",
        "    loser_rank = get_elo(season, lose_team)\n",
        "\n",
        "    \"\"\"\n",
        "    This is originally from from:\n",
        "    http://zurb.com/forrst/posts/An_Elo_Rating_function_in_Python_written_for_foo-hQl\n",
        "    \"\"\"\n",
        "    \n",
        "    rank_diff = winner_rank - loser_rank\n",
        "    exp = (rank_diff * -1) / 400\n",
        "    odds = 1 / (1 + math.pow(10, exp))\n",
        "    if winner_rank < 2100:\n",
        "        k = 32\n",
        "    elif winner_rank >= 2100 and winner_rank < 2400:\n",
        "        k = 24\n",
        "    else:\n",
        "        k = 16\n",
        "    new_winner_rank = round(winner_rank + (k * (1 - odds)))\n",
        "    new_rank_diff = new_winner_rank - winner_rank\n",
        "    new_loser_rank = loser_rank - new_rank_diff\n",
        "\n",
        "    return new_winner_rank, new_loser_rank\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b0e4688",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_stat(season, team, field):\n",
        "    try:\n",
        "        l = team_stats[season][team][field]\n",
        "        return sum(l) / float(len(l))\n",
        "    except:\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5135d5de",
      "metadata": {},
      "outputs": [],
      "source": [
        "# This function is used to predict the winner of two teams in a head to head match up. \n",
        "# An array called features is created locally, the next step is to collect the elo rating and \n",
        "# stats of that team. \n",
        "def predict_winner(team_1, team_2, model, season, stat_fields):\n",
        "    features = []\n",
        "\n",
        "    # Team 1\n",
        "    features.append(get_elo(season, team_1))\n",
        "    for stat in stat_fields:\n",
        "        features.append(get_stat(season, team_1, stat))\n",
        "\n",
        "    # Team 2\n",
        "    features.append(get_elo(season, team_2))\n",
        "    for stat in stat_fields:\n",
        "        features.append(get_stat(season, team_2, stat))\n",
        "\n",
        "    return model.predict_proba([features])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0176ab26",
      "metadata": {},
      "outputs": [],
      "source": [
        "def update_stats(season, team, fields):\n",
        "    \"\"\"\n",
        "    This accepts some stats for a team and udpates the averages.\n",
        "    First, we check if the team is in the dict yet. If it's not, we add it.\n",
        "    Then, we try to check if the key has more than 5 values in it.\n",
        "        If it does, we remove the first one\n",
        "        Either way, we append the new one.\n",
        "    If we can't check, then it doesn't exist, so we just add this.\n",
        "    Later, we'll get the average of these items.\n",
        "    \"\"\"\n",
        "    if team not in team_stats[season]:\n",
        "        team_stats[season][team] = {}\n",
        "\n",
        "    for key, value in fields.items():\n",
        "        # Make sure we have the field.\n",
        "        if key not in team_stats[season][team]:\n",
        "            team_stats[season][team][key] = []\n",
        "\n",
        "        if len(team_stats[season][team][key]) >= 9:\n",
        "            team_stats[season][team][key].pop()\n",
        "        team_stats[season][team][key].append(value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e21c4336",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_team_dict():\n",
        "    team_ids = seeds\n",
        "    team_id_map = {}\n",
        "    for index, row in team_ids.iterrows():\n",
        "        team_id_map[row['TeamID']] = row['TeamName']\n",
        "    return team_id_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51afd958",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_season_data(all_data):\n",
        "    \n",
        "    # Calculate the elo rating for every game for every team, each season.\n",
        "    # Store the elo per season so we can retrieve their end elo\n",
        "    # later in order to predict the tournaments without having to\n",
        "    # inject the prediction into this loop.\n",
        "    print(\"Building season data.\")\n",
        "    for index, row in all_data.iterrows():\n",
        "        # Used to skip matchups where we don't have usable stats yet.\n",
        "        skip = 0\n",
        "\n",
        "        # Get starter or previous elos.\n",
        "        team_1_elo = get_elo(row['Season'], row['WTeamID'])\n",
        "        team_2_elo = get_elo(row['Season'], row['LTeamID'])\n",
        "\n",
        "        # Add 100 to the home team (# taken from Nate Silver analysis.) \n",
        "        # Nate Silver is one of the founders of FiveThirtyEight and initiated a\n",
        "        # data driven approach to predicting the team with the highest probability \n",
        "        # of winning March Madness. This line of code quantifies \"home court advantage\"\n",
        "\n",
        "        if row['WLoc'] == 'H':\n",
        "            team_1_elo += 100\n",
        "        elif row['WLoc'] == 'A':\n",
        "            team_2_elo += 100\n",
        "\n",
        "        # We'll create some arrays to use later.\n",
        "        team_1_features = [team_1_elo]\n",
        "        team_2_features = [team_2_elo]\n",
        "\n",
        "        # Build arrays out of the stats we're tracking.\n",
        "        for field in stat_fields:\n",
        "            team_1_stat = get_stat(row['Season'], row['WTeamID'], field)\n",
        "            team_2_stat = get_stat(row['Season'], row['LTeamID'], field)\n",
        "            if team_1_stat != 0 and team_2_stat != 0:\n",
        "                team_1_features.append(team_1_stat)\n",
        "                team_2_features.append(team_2_stat)\n",
        "            else:\n",
        "                skip = 1\n",
        "\n",
        "        if skip == 0:  # Make sure we have stats.\n",
        "            # Randomly select left and right and 0 or 1 so we can train\n",
        "            # for multiple classes.\n",
        "            if random.random() > 0.5:\n",
        "                X.append(team_1_features + team_2_features)\n",
        "                y.append(0)\n",
        "            else:\n",
        "                X.append(team_2_features + team_1_features)\n",
        "                y.append(1)\n",
        "\n",
        "        # AFTER we add the current stuff to the prediction, update for\n",
        "        # next time. Order here is key so we don't fit on data from the\n",
        "        # same game we're trying to predict.\n",
        "        if row['WFTA'] != 0 and row['LFTA'] != 0:\n",
        "            stat_1_fields = {\n",
        "                'score': row['WScore'],\n",
        "                'fgp': row['WFGM'] / row['WFGA'] * 100,\n",
        "                'fga': row['WFGA'],\n",
        "                'fga3': row['WFGA3'],\n",
        "                '3pp': row['WFGM3'] / row['WFGA3'] * 100,\n",
        "                'ftp': row['WFTM'] / row['WFTA'] * 100,\n",
        "                'or': row['WOR'],\n",
        "                'dr': row['WDR'],\n",
        "                'ast': row['WAst'],\n",
        "                'to': row['WTO'],\n",
        "                'stl': row['WStl'],\n",
        "                'blk': row['WBlk'],\n",
        "                'pf': row['WPF']\n",
        "            }\n",
        "            stat_2_fields = {\n",
        "                'score': row['LScore'],\n",
        "                'fgp': row['LFGM'] / row['LFGA'] * 100,\n",
        "                'fga': row['LFGA'],\n",
        "                'fga3': row['LFGA3'],\n",
        "                '3pp': row['LFGM3'] / row['LFGA3'] * 100,\n",
        "                'ftp': row['LFTM'] / row['LFTA'] * 100,\n",
        "                'or': row['LOR'],\n",
        "                'dr': row['LDR'],\n",
        "                'ast': row['LAst'],\n",
        "                'to': row['LTO'],\n",
        "                'stl': row['LStl'],\n",
        "                'blk': row['LBlk'],\n",
        "                'pf': row['LPF']\n",
        "            }\n",
        "            update_stats(row['Season'], row['WTeamID'], stat_1_fields)\n",
        "            update_stats(row['Season'], row['LTeamID'], stat_2_fields)\n",
        "\n",
        "        # Now that we've added them, calc the new elo.\n",
        "        new_winner_rank, new_loser_rank = calc_elo(\n",
        "            row['WTeamID'], row['LTeamID'], row['Season'])\n",
        "        team_elos[row['Season']][row['WTeamID']] = new_winner_rank\n",
        "        team_elos[row['Season']][row['LTeamID']] = new_loser_rank\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e17f2fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    stat_fields = ['score', 'fga', 'fgp', 'fga3', '3pp', 'ftp', 'or', 'dr',\n",
        "                   'ast', 'to', 'stl', 'blk', 'pf']\n",
        "\n",
        "    initialize_data()\n",
        "    season_data = mm_regseason\n",
        "    tourney_data = mm_tourney\n",
        "    frames = [season_data, tourney_data]\n",
        "    all_data = pd.concat(frames)\n",
        "\n",
        "    # Build the working data.\n",
        "    X, y = build_season_data(all_data)\n",
        "\n",
        "    # Fit the model with our training data. \n",
        "    # The newton-cholesky solver was used because according to sci-kit learn documentation\n",
        "    # this solver is ideal for classifcation problems where the number of samples (1248) is\n",
        "    # much larger than the number of features (34)\n",
        "\n",
        "    print(\"Fitting on %d samples.\" % len(X))\n",
        "    model = LogisticRegression(solver='newton-cholesky')\n",
        "    model.fit(X, y)\n",
        "    kfold = KFold(n_splits=5, random_state=0, shuffle=True)\n",
        "    results = cross_val_score(model, X, y, cv=kfold)\n",
        "    \n",
        "    # Output the accuracy. Calculate the mean and std across all folds.\n",
        "    print(\"Accuracy: %.3f%% (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))\n",
        "\n",
        "    # Now predict tournament matchups.\n",
        "    print(\"Getting teams.\")\n",
        "    # for i in range(2016, 2017):\n",
        "    tourney_teams = []\n",
        "    for index, row in seeds.iterrows():\n",
        "        if row['Season'] == prediction_year:\n",
        "            tourney_teams.append(row['TeamID'])\n",
        "\n",
        "    # Build our prediction of every matchup.\n",
        "    print(\"Predicting matchups.\")\n",
        "    tourney_teams.sort()\n",
        "    for team_1 in tourney_teams:\n",
        "        for team_2 in tourney_teams:\n",
        "            if team_1 < team_2:\n",
        "                prediction = predict_winner(\n",
        "                    team_1, team_2, model, prediction_year, stat_fields)\n",
        "                label = str(prediction_year) + '_' + str(team_1) + '_' + \\\n",
        "                    str(team_2)\n",
        "                submission_data.append([label, prediction[0][0]])\n",
        "\n",
        "\n",
        "                # Write the results.\n",
        "    print(\"Writing %d results.\" % len(submission_data))\n",
        "    with open(folder + '/submission.csv', 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['id', 'pred'])\n",
        "        writer.writerows(submission_data)\n",
        "\n",
        "    # Now so that we can use this to fill out a bracket, create a readable\n",
        "    # version.\n",
        "    print(\"Outputting readable results.\")\n",
        "    team_id_map = build_team_dict()\n",
        "    readable = []\n",
        "    less_readable = []  # A version that's easy to look up.\n",
        "    for pred in submission_data:\n",
        "        parts = pred[0].split('_')\n",
        "        less_readable.append(\n",
        "            [team_id_map[float(parts[1])], team_id_map[float(parts[2])], pred[1]])\n",
        "        # Order them properly.\n",
        "        if pred[1] > 0.5:\n",
        "            winning = float(parts[1])\n",
        "            losing = float(parts[2])\n",
        "            proba = pred[1]\n",
        "        else:\n",
        "            winning = float(parts[2])\n",
        "            losing = float(parts[1])\n",
        "            proba = 1 - pred[1]\n",
        "        readable.append(\n",
        "            [\n",
        "                '%s beats %s: %f' %\n",
        "                (team_id_map[winning], team_id_map[losing], proba)\n",
        "            ]\n",
        "        )\n",
        "    with open(folder + '/readable-predictions.csv', 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerows(readable)\n",
        "    with open(folder + '/less-readable-predictions.csv', 'w') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerows(less_readable)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "338bc9c6",
      "metadata": {
        "id": "338bc9c6"
      },
      "source": [
        "Then we want to select any variables that we do not think will be helpful. In this case, I am choosing to remove the year variables, as I only want to focus on a team's game stats, and not the year that they took place, in order to predict how far teams will go long term in the game. You can select what variables you want to include in your model by thinking through your assumptions about what variables are most highly correlated. You can even use a correlation matrix to best determine the highest correlations between each variable and your target variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5cbc4fb",
      "metadata": {
        "id": "b5cbc4fb"
      },
      "outputs": [],
      "source": [
        "#mm = dummies.drop('YEAR', axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d173f1a",
      "metadata": {
        "id": "5d173f1a"
      },
      "source": [
        "Now we want to consider what method of classification we are going to use. There are many different types classification methods. The following are just a subset of them:\n",
        "- Decision Tree/Random Forest: dataset attributes become nodes or branches of a tree \n",
        "    - Pros: works well with both numerical and categorical data, implicitly performs feature selection, not greatly influenced by outliers\n",
        "    - Cons: not easily interpretable, computationally intensive\n",
        "- K-Nearest Neighbors: creates groups based off of the k-nearest values to some centroid\n",
        "    - Pros: very simple, easy to implement for multi-class problems, many distance variables to choose from\n",
        "    - Cons: very sensitive to outliers, slow and performs worse for higher dimensions of data\n",
        "- Support Vector Machines: model with associated learning algorithms that analyze data for classifications\n",
        "    - Pros: works well with clear margin of separation, effective in high dimensions\n",
        "    - Cons: doesn't perform well on noisy data, no probability estimates"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a89e569",
      "metadata": {
        "id": "8a89e569"
      },
      "source": [
        "For this demonstration, we are going to work with a random forest algorithm. We need to start by determining what the best parameters are based on our data for our random forest algorithm. To do this we are going to create a pipeline that contains the algorithm and parameters we want to train, followed by a grid search which will test a variety of values for each and return the best fit on the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e08c3721",
      "metadata": {
        "id": "e08c3721"
      },
      "outputs": [],
      "source": [
        "#from sklearn.pipeline import Pipeline\n",
        "#from sklearn.model_selection import GridSearchCV\n",
        "#from sklearn.ensemble import RandomForestClassifier\n",
        "#from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65855864",
      "metadata": {
        "id": "65855864"
      },
      "outputs": [],
      "source": [
        "#pipe = Pipeline([['reduction', PCA()], ['classify', RandomForestClassifier()]])\n",
        "#pipe_grid = { 'reduction__n_components': [2, 5, 8, 10], 'classify__n_estimators': [10, 100, 1000], 'classify__criterion': [\"gini\", \"entropy\", \"log_loss\"]}\n",
        "#gs = GridSearchCV(pipe, pipe_grid)\n",
        "#gs.fit(X_train, y_train)\n",
        "#best_parameters = gs.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ffdc5b0",
      "metadata": {
        "id": "6ffdc5b0"
      },
      "outputs": [],
      "source": [
        "#best_parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d031a78b",
      "metadata": {
        "id": "d031a78b"
      },
      "source": [
        "Once we have the best parameters, we want to be able to test our model using our X_test and y_test values. We start by reducing the dimensions of our data using the best computed parameter. We then fit the X_train and y_train data to the RandomForestClassifier with the values that we received from the GridSearchCV best parameters. We then predict on the X_test data to get a prediction for y."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff61eb0f",
      "metadata": {
        "id": "ff61eb0f"
      },
      "outputs": [],
      "source": [
        "#pca = PCA(n_components = best_parameters['reduction__n_components'])\n",
        "#X_train = pca.fit_transform(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb7a89d6",
      "metadata": {
        "id": "bb7a89d6"
      },
      "outputs": [],
      "source": [
        "#model = RandomForestClassifier(n_estimators = best_parameters['classify__n_estimators'], criterion = best_parameters['classify__criterion'])\n",
        "#fitted_model = model.fit(X_train, y_train)\n",
        "#y_predicted = fitted_model.predict(pca.fit_transform(X_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c35f7c2",
      "metadata": {
        "id": "6c35f7c2"
      },
      "source": [
        "Now we want to test how well our algorithm performed on our testing data. We can do this in many different ways. One way is by computing the accuracy and precision scores using the sklearn.metrics library. The definition of both are as follows:\n",
        "- Accuracy Score: an evaluation metric that measures the number of correct predictions made by a model in relation to the total number of predictions made\n",
        "- Precision Score: the ratio of true positive to the sum of true positive and false positive"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "504fefec",
      "metadata": {
        "id": "504fefec"
      },
      "source": [
        "Another great way to determine the intricacies of how our model is doing is by generating the confusion matrix which shows both how many times the model got the right answer for each classification, but also how many misclassifications occured per category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e33592c4",
      "metadata": {
        "id": "e33592c4"
      },
      "outputs": [],
      "source": [
        "#from sklearn.metrics import confusion_matrix, accuracy_score, precision_score\n",
        "#cm = confusion_matrix(y_test, y_predicted, labels = mm['POSTSEASON'].unique())\n",
        "#acc = accuracy_score(y_test, y_predicted)\n",
        "#prec = precision_score(y_test, y_predicted, average = \"weighted\")\n",
        "#print(\"Accuracy Score: \" + str(acc))\n",
        "#print(\"Precision Score: \" + str(prec))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d129e5c6",
      "metadata": {
        "id": "d129e5c6"
      },
      "outputs": [],
      "source": [
        "#sb.heatmap(cm, annot = True, cmap=\"Blues\", xticklabels = mm['POSTSEASON'].unique(), yticklabels = mm['POSTSEASON'].unique())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
